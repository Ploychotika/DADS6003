{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Student id**: 6410422016\n\n**Name**: Chotika Boonthaweechok\n\n**Nickname**: Ploy\n\n**Rank**: 1\n\n**Public Score**: 0.99257\n\n**Private Score**: 0.98933\n\n**Model**: QuadraticDiscriminantAnalysis\n","metadata":{}},{"cell_type":"markdown","source":"# **üíô Introduction**\n\nThis was my work to competition for my class. The data are about binary classification. This work represents a deeper analysis by playing on several parameters. I will do a lot of work. You can follow this and let's go. \n\nThe following kernel contains the steps enumerated below for assessing the dataset:\n\n1. [Import data and python packages](#t1.)\n2. [Cleaning data](#t2.)\n3. [Data exploration after cleaning data](#t3.)\n4. [Feature Selection](#t4.)\n5. [Model section](#t5.)\n6. [Test and Submission](#t6.)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"t1.\"></a>\n# ‚ù§Ô∏è  1. Import Data & Python Packages","metadata":{}},{"cell_type":"code","source":"# data analysis\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno \n\n# machine learning only sklearn for this rule\nimport sklearn.linear_model as lm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc, f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neural_network import MLPClassifier\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-13T12:37:07.913519Z","iopub.execute_input":"2022-06-13T12:37:07.913900Z","iopub.status.idle":"2022-06-13T12:37:09.695594Z","shell.execute_reply.started":"2022-06-13T12:37:07.913799Z","shell.execute_reply":"2022-06-13T12:37:09.694492Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Acquire data\n\nStart by acquiring the training, testing and submission datasets into Pandas DataFrames.\n\nThe several key items in this folder:\n\n* train.csv: CSV file containing training set and labels (including y value)\n* test.csv: CSV file containing testing set and labels (not including y value)\n* sampleSubmission.csv: CSV file that we should input predict data to this file","metadata":{}},{"cell_type":"code","source":"# Read CSV train data file into DataFrame\ndf_train = pd.read_csv('/kaggle/input/dads6003-in-class-competition/train.csv')\n\n# Read CSV test data file into DataFrame\ndf_test = pd.read_csv('/kaggle/input/dads6003-in-class-competition/test.csv')\n\n# Read CSV submission file\nsubmission = pd.read_csv('/kaggle/input/dads6003-in-class-competition/sampleSubmission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.697745Z","iopub.execute_input":"2022-06-13T12:37:09.698031Z","iopub.status.idle":"2022-06-13T12:37:09.794975Z","shell.execute_reply.started":"2022-06-13T12:37:09.697997Z","shell.execute_reply":"2022-06-13T12:37:09.794027Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Analyze by describing data\n\nGoing to display five data sample for training and testing dataset. There train data is 7500 observations with 21 features (x1-x20 and y). These test data is 2500 observations with 20 features (x1-x20). All data are within numerical features (mean variables for which the values are numbers). x1 - x20 are float64 but y is int64.","metadata":{}},{"cell_type":"code","source":"# preview train data\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.796323Z","iopub.execute_input":"2022-06-13T12:37:09.796594Z","iopub.status.idle":"2022-06-13T12:37:09.834324Z","shell.execute_reply.started":"2022-06-13T12:37:09.796563Z","shell.execute_reply":"2022-06-13T12:37:09.833302Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('There df_train is {} observations with {} features'.format(df_train.shape[0], df_train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.835589Z","iopub.execute_input":"2022-06-13T12:37:09.835809Z","iopub.status.idle":"2022-06-13T12:37:09.840921Z","shell.execute_reply.started":"2022-06-13T12:37:09.835784Z","shell.execute_reply":"2022-06-13T12:37:09.840056Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# preview test data\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.843667Z","iopub.execute_input":"2022-06-13T12:37:09.844475Z","iopub.status.idle":"2022-06-13T12:37:09.870861Z","shell.execute_reply.started":"2022-06-13T12:37:09.844369Z","shell.execute_reply":"2022-06-13T12:37:09.869610Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print('There df_test is {} observations with {} features'.format(df_test.shape[0], df_test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.873692Z","iopub.execute_input":"2022-06-13T12:37:09.874690Z","iopub.status.idle":"2022-06-13T12:37:09.880961Z","shell.execute_reply.started":"2022-06-13T12:37:09.874651Z","shell.execute_reply":"2022-06-13T12:37:09.880141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.882231Z","iopub.execute_input":"2022-06-13T12:37:09.882451Z","iopub.status.idle":"2022-06-13T12:37:09.912323Z","shell.execute_reply.started":"2022-06-13T12:37:09.882427Z","shell.execute_reply":"2022-06-13T12:37:09.911438Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"cols = ['x1','x2', 'x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20']\nx_train = df_train[cols]\ny_train = df_train['y']\ndf_train[cols].describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.913826Z","iopub.execute_input":"2022-06-13T12:37:09.914061Z","iopub.status.idle":"2022-06-13T12:37:09.987676Z","shell.execute_reply.started":"2022-06-13T12:37:09.914034Z","shell.execute_reply":"2022-06-13T12:37:09.986776Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:09.988772Z","iopub.execute_input":"2022-06-13T12:37:09.988978Z","iopub.status.idle":"2022-06-13T12:37:10.054839Z","shell.execute_reply.started":"2022-06-13T12:37:09.988947Z","shell.execute_reply":"2022-06-13T12:37:10.051518Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t2.\"></a>\n# ‚ù§Ô∏è  2. Cleaning data","metadata":{}},{"cell_type":"markdown","source":"1) **Checking for outlier**: The outlier is is the value which is out of the range. If I didn't clean this value, mean and Standard Deviation won't correctly. Outlier detection have a several methods but I will use \"**IQR METHOD**\" In this method by using Inter Quartile Range(IQR), which is beyond the range of -1.5 x IQR to 1.5 x IQR treated as outliers. I will drop outlier on the training data ","metadata":{}},{"cell_type":"code","source":"def iqr_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    print(f\"Lower_tail: {Lower_tail}\")\n    print(f\"Upper_tail: {Upper_tail}\")\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:10.056444Z","iopub.execute_input":"2022-06-13T12:37:10.057242Z","iopub.status.idle":"2022-06-13T12:37:10.064904Z","shell.execute_reply.started":"2022-06-13T12:37:10.057157Z","shell.execute_reply":"2022-06-13T12:37:10.063627Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cols = ['x1','x2', 'x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20']\nfor x in cols:\n    print(x)\n    iqr_outliers(df_train[x])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:10.066671Z","iopub.execute_input":"2022-06-13T12:37:10.067744Z","iopub.status.idle":"2022-06-13T12:37:10.150072Z","shell.execute_reply.started":"2022-06-13T12:37:10.067696Z","shell.execute_reply":"2022-06-13T12:37:10.149301Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cols = ['x1','x2', 'x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20']\nfor x in cols:\n    print(f'Plots {x}')\n    sns.boxplot(df_train[x])\n    plt.title(\"Outlier according to parameters\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:10.151534Z","iopub.execute_input":"2022-06-13T12:37:10.151871Z","iopub.status.idle":"2022-06-13T12:37:13.512377Z","shell.execute_reply.started":"2022-06-13T12:37:10.151828Z","shell.execute_reply":"2022-06-13T12:37:13.511619Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Actions**: Remove outlier on training data as below.","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(df_train[df_train.x1 < -23.12294316].index)\ndf_train = df_train.drop(df_train[df_train.x1 > -6.9373999975].index)\ndf_train = df_train.drop(df_train[df_train.x2 < -422.980552].index)\ndf_train = df_train.drop(df_train[df_train.x2 > -16.61602476249996].index)\ndf_train = df_train.drop(df_train[df_train.x3 < 3.1069829375000104].index)\ndf_train = df_train.drop(df_train[df_train.x3 > 84.96017941749999].index)\ndf_train = df_train.drop(df_train[df_train.x4 < 2.3923348597499996].index)\ndf_train = df_train.drop(df_train[df_train.x4 > 7.64356002175].index)\ndf_train = df_train.drop(df_train[df_train.x5 < 2.269501245124998].index)\ndf_train = df_train.drop(df_train[df_train.x5 > 7.7232435701250015].index)\ndf_train = df_train.drop(df_train[df_train.x6 < 79.46606857499984].index)\ndf_train = df_train.drop(df_train[df_train.x6 > 1409.8497135750001].index)\ndf_train = df_train.drop(df_train[df_train.x7 < -537.3616765749998].index)\ndf_train = df_train.drop(df_train[df_train.x7 > -64.63595957500007].index)\ndf_train = df_train.drop(df_train[df_train.x8 < 4.627275825000002].index)\ndf_train = df_train.drop(df_train[df_train.x8 > 31.384570104999998].index)\ndf_train = df_train.drop(df_train[df_train.x9 < 2515.969874999988].index)\ndf_train = df_train.drop(df_train[df_train.x9 > 108121.76665500001].index)\ndf_train = df_train.drop(df_train[df_train.x10 < 8.545773954999998].index)\ndf_train = df_train.drop(df_train[df_train.x10 > 51.867057515].index)\ndf_train = df_train.drop(df_train[df_train.x11 < 5.765985397499998].index)\ndf_train = df_train.drop(df_train[df_train.x11 > 19.2815462175].index)\ndf_train = df_train.drop(df_train[df_train.x12 < -0.7274577066250001].index)\ndf_train = df_train.drop(df_train[df_train.x12 > 8.694357346375].index)\ndf_train = df_train.drop(df_train[df_train.x13 < 2296.1878593750002].index)\ndf_train = df_train.drop(df_train[df_train.x13 > 7652.747208375].index)\ndf_train = df_train.drop(df_train[df_train.x14 < 29.316874212499997].index)\ndf_train = df_train.drop(df_train[df_train.x14 > 210.23596799250004].index)\ndf_train = df_train.drop(df_train[df_train.x15 < 1.8434734250000009].index)\ndf_train = df_train.drop(df_train[df_train.x15 > 31.393640345].index)\ndf_train = df_train.drop(df_train[df_train.x16 < 18.400854194999997].index)\ndf_train = df_train.drop(df_train[df_train.x16 > 61.560120874999996].index)\ndf_train = df_train.drop(df_train[df_train.x17 < 91.280765875].index)\ndf_train = df_train.drop(df_train[df_train.x17 > 310.610162475].index)\ndf_train = df_train.drop(df_train[df_train.x18 < 4.789812958124999].index)\ndf_train = df_train.drop(df_train[df_train.x18 > 15.233088717125002].index)\ndf_train = df_train.drop(df_train[df_train.x19 < 116.15875480000007].index)\ndf_train = df_train.drop(df_train[df_train.x19 > 391.3999163999999].index)\ndf_train = df_train.drop(df_train[df_train.x20 < -7.63674662325].index)\ndf_train = df_train.drop(df_train[df_train.x20 > -2.39640915925].index)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:13.513489Z","iopub.execute_input":"2022-06-13T12:37:13.514145Z","iopub.status.idle":"2022-06-13T12:37:13.585435Z","shell.execute_reply.started":"2022-06-13T12:37:13.514084Z","shell.execute_reply":"2022-06-13T12:37:13.584862Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print('There df_train after remove outlier is {} observations with {} features'.format(df_train.shape[0], df_train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:13.588039Z","iopub.execute_input":"2022-06-13T12:37:13.589140Z","iopub.status.idle":"2022-06-13T12:37:13.594064Z","shell.execute_reply.started":"2022-06-13T12:37:13.589106Z","shell.execute_reply":"2022-06-13T12:37:13.593507Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"2) **Checking for Missing value**: Only training data have missing value. I should handle missing value before training model. I can't train model like this. I will change null value to mean or median. All data expect x2 of mean and median are the same. So I will use only mean instead of median but for x2 I will use median because it's high standard deviation.","metadata":{}},{"cell_type":"code","source":"print('Train columns with null values:\\n', df_train.isnull().sum())\nprint(\"-\"*10)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:13.599069Z","iopub.execute_input":"2022-06-13T12:37:13.599606Z","iopub.status.idle":"2022-06-13T12:37:13.612708Z","shell.execute_reply.started":"2022-06-13T12:37:13.599558Z","shell.execute_reply":"2022-06-13T12:37:13.611675Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"msno.bar(df_train) ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:13.614455Z","iopub.execute_input":"2022-06-13T12:37:13.615254Z","iopub.status.idle":"2022-06-13T12:37:15.011416Z","shell.execute_reply.started":"2022-06-13T12:37:13.615210Z","shell.execute_reply":"2022-06-13T12:37:15.010427Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print('Test/Validation columns with null values:\\n', df_test.isnull().sum())\nprint(\"-\"*10)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:15.013192Z","iopub.execute_input":"2022-06-13T12:37:15.014271Z","iopub.status.idle":"2022-06-13T12:37:15.024433Z","shell.execute_reply.started":"2022-06-13T12:37:15.014213Z","shell.execute_reply":"2022-06-13T12:37:15.023445Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_train[cols].agg(['mean','median', 'std'], axis=\"rows\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:15.026419Z","iopub.execute_input":"2022-06-13T12:37:15.027098Z","iopub.status.idle":"2022-06-13T12:37:15.093338Z","shell.execute_reply.started":"2022-06-13T12:37:15.027043Z","shell.execute_reply":"2022-06-13T12:37:15.092295Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train[\"x1\"].fillna(df_train[\"x1\"].mean(), inplace = True)\ndf_train[\"x2\"].fillna(df_train[\"x2\"].median(), inplace = True)\ndf_train[\"x3\"].fillna(df_train[\"x3\"].mean(), inplace = True)\ndf_train[\"x4\"].fillna(df_train[\"x4\"].mean(), inplace = True)\ndf_train[\"x5\"].fillna(df_train[\"x5\"].mean(), inplace = True)\ndf_train[\"x6\"].fillna(df_train[\"x6\"].mean(), inplace = True)\ndf_train[\"x7\"].fillna(df_train[\"x7\"].mean(), inplace = True)\ndf_train[\"x8\"].fillna(df_train[\"x8\"].mean(), inplace = True)\ndf_train[\"x9\"].fillna(df_train[\"x9\"].mean(), inplace = True)\ndf_train[\"x10\"].fillna(df_train[\"x10\"].mean(), inplace = True)\ndf_train[\"x11\"].fillna(df_train[\"x11\"].mean(), inplace = True)\ndf_train[\"x12\"].fillna(df_train[\"x12\"].mean(), inplace = True)\ndf_train[\"x13\"].fillna(df_train[\"x13\"].mean(), inplace = True)\ndf_train[\"x14\"].fillna(df_train[\"x14\"].mean(), inplace = True)\ndf_train[\"x15\"].fillna(df_train[\"x15\"].mean(), inplace = True)\ndf_train[\"x16\"].fillna(df_train[\"x16\"].mean(), inplace = True)\ndf_train[\"x17\"].fillna(df_train[\"x17\"].mean(), inplace = True)\ndf_train[\"x18\"].fillna(df_train[\"x18\"].mean(), inplace = True)\ndf_train[\"x19\"].fillna(df_train[\"x19\"].mean(), inplace = True)\ndf_train[\"x20\"].fillna(df_train[\"x20\"].mean(), inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:15.095051Z","iopub.execute_input":"2022-06-13T12:37:15.095698Z","iopub.status.idle":"2022-06-13T12:37:15.120807Z","shell.execute_reply.started":"2022-06-13T12:37:15.095651Z","shell.execute_reply":"2022-06-13T12:37:15.120107Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t3.\"></a>\n# ‚ù§Ô∏è  3. Data exploration","metadata":{}},{"cell_type":"markdown","source":"1.1.) Explore Deposit Result (0/1)\n\nAs graph, The zero value has more than the 1 value. It's almost 3 times of 1.","metadata":{}},{"cell_type":"code","source":"n, y = (df_train['y'].value_counts())\ndf_train['y'].value_counts().plot.bar(color=['darkred','darkblue'], title = 'Deposit value counts');\nprint('Number of 0: ',n)\nprint('Number of 1 : ',y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:15.121927Z","iopub.execute_input":"2022-06-13T12:37:15.124068Z","iopub.status.idle":"2022-06-13T12:37:15.320230Z","shell.execute_reply.started":"2022-06-13T12:37:15.124015Z","shell.execute_reply":"2022-06-13T12:37:15.319338Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t4.\"></a>\n# ‚ù§Ô∏è  4. Feature Selection\n\nWe have many parameters on this data but need to analysis what is the important feature. If use all parameters, model won't get high accuracy and can't predict it. In the first, I will use Feature selection with correlation to select features.\n\n**1. Feature selection with correlation**\n\n* x6, x9, x14 are correlated with each other and I only use x9.\n* x10, x15 are correlated with each other and I only use x15.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df_train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:15.323980Z","iopub.execute_input":"2022-06-13T12:37:15.325086Z","iopub.status.idle":"2022-06-13T12:37:17.484161Z","shell.execute_reply.started":"2022-06-13T12:37:15.325034Z","shell.execute_reply":"2022-06-13T12:37:17.482935Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"I will try to run model and see accuracy. I try to choose \"random forest classification\" first. Accuracy is almost 93% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","metadata":{}},{"cell_type":"code","source":"colsSelect1 = ['x1','x2', 'x3','x4','x5','x6','x7','x8','x9','x11','x12','x13','x15','x16','x17','x18','x19','x20']\n\nx_train = np.array(df_train[colsSelect1])                \ny_train = np.array(df_train[\"y\"]) \nx_test = np.array(df_test[colsSelect1]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:17.485661Z","iopub.execute_input":"2022-06-13T12:37:17.485922Z","iopub.status.idle":"2022-06-13T12:37:17.495604Z","shell.execute_reply.started":"2022-06-13T12:37:17.485890Z","shell.execute_reply":"2022-06-13T12:37:17.494576Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,test_size=0.3,random_state=1)\nmodel=RandomForestClassifier(criterion='entropy', max_depth=10, max_features='sqrt',\n                       min_samples_split=3, n_estimators=900)\nmodel.fit(x_train, y_train)\nforest_pred = model.predict(x_test)\nprint('\\n',confusion_matrix(y_test, forest_pred))\nprint('CLASSIFICATION REPORT FOR FEATURE SELECT1\\n\\n', classification_report(y_test, forest_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:17.497540Z","iopub.execute_input":"2022-06-13T12:37:17.497778Z","iopub.status.idle":"2022-06-13T12:37:32.998578Z","shell.execute_reply.started":"2022-06-13T12:37:17.497750Z","shell.execute_reply":"2022-06-13T12:37:32.997352Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**2. Feature Importances**\n\nFor feature importance, I choose \"colsSelect2\" value and f1-score with weighted avg is '0.97'. It's more than Feature selection with correlation.","metadata":{}},{"cell_type":"code","source":"colsSelect2 = ['x1','x2', 'x3','x4','x5','x6','x7','x8','x9','x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20']\nx_train = np.array(df_train[colsSelect2])                \ny_train = np.array(df_train[\"y\"])   ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:33.000143Z","iopub.execute_input":"2022-06-13T12:37:33.000391Z","iopub.status.idle":"2022-06-13T12:37:33.008376Z","shell.execute_reply.started":"2022-06-13T12:37:33.000362Z","shell.execute_reply":"2022-06-13T12:37:33.007493Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = ExtraTreesClassifier()\nmodel.fit(x_train,y_train)\n\nfeat_importances = pd.Series(model.feature_importances_, index=colsSelect2)\nfeat_importances.nlargest(20).plot(kind='bar',color='darkblue')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:33.009501Z","iopub.execute_input":"2022-06-13T12:37:33.009826Z","iopub.status.idle":"2022-06-13T12:37:33.906118Z","shell.execute_reply.started":"2022-06-13T12:37:33.009782Z","shell.execute_reply":"2022-06-13T12:37:33.905361Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"colsSelect2 = ['x2', 'x3','x6','x7','x8','x9','x10','x12','x14','x15']\nx_train = np.array(df_train[colsSelect2])                \ny_train = np.array(df_train[\"y\"])   ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:33.907177Z","iopub.execute_input":"2022-06-13T12:37:33.908174Z","iopub.status.idle":"2022-06-13T12:37:33.914308Z","shell.execute_reply.started":"2022-06-13T12:37:33.908133Z","shell.execute_reply":"2022-06-13T12:37:33.913572Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,test_size=0.3,random_state=1)\nmodel=RandomForestClassifier(criterion='entropy', max_depth=10, max_features='sqrt',\n                       min_samples_split=3, n_estimators=900)\nmodel.fit(x_train, y_train)\nforest_pred = model.predict(x_test)\nprint('\\n',confusion_matrix(y_test, forest_pred))\nprint('CLASSIFICATION REPORT FOR FEATURE SELECT1\\n\\n', classification_report(y_test, forest_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:33.915973Z","iopub.execute_input":"2022-06-13T12:37:33.917139Z","iopub.status.idle":"2022-06-13T12:37:45.754352Z","shell.execute_reply.started":"2022-06-13T12:37:33.917097Z","shell.execute_reply":"2022-06-13T12:37:45.753276Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t5.\"></a>\n# ‚ù§Ô∏è  5. Model selection on scikit-learn\n\nWe've established a basic idea about this data so build to choose model. This model that we try to train and test data as below. We select the one that have high accuracy.\n\n* LinearDiscriminantAnalysis\n* QuadraticDiscriminantAnalysis\n* AdaBoostClassifier\n* BaggingClassifier\n* ExtraTreesClassifier\n* GradientBoostingClassifier\n* RandomForestClassifier\n* RidgeClassifier\n* SGDClassifier\n* BernoulliNB\n* GaussianNB\n* KNeighborsClassifier\n* MLPClassifier\n* LinearSVC\n* NuSVC\n* DecisionTreeClassifier\n* ExtraTreeClassifier\n\nFinal the first choose is QuadraticDiscriminantAnalysis and second choose is RandomForestClassifier.","metadata":{}},{"cell_type":"code","source":"colsfinal = ['x2', 'x3','x6','x7','x8','x9','x10','x12','x14','x15']\nx_train1 = np.array(df_train[colsfinal])                \ny_train1 = np.array(df_train[\"y\"]) \nx_test1 = np.array(df_test[colsfinal]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:45.758989Z","iopub.execute_input":"2022-06-13T12:37:45.759347Z","iopub.status.idle":"2022-06-13T12:37:45.768948Z","shell.execute_reply.started":"2022-06-13T12:37:45.759310Z","shell.execute_reply":"2022-06-13T12:37:45.767495Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"models = []\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('AdaBoost', AdaBoostClassifier()))\nmodels.append(('Bagging', BaggingClassifier()))\nmodels.append(('Extra Trees Ensemble', ExtraTreesClassifier(n_estimators=1000)))\nmodels.append(('Gradient Boosting', GradientBoostingClassifier()))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=1000)))\nmodels.append(('Ridge', RidgeClassifier()))\nmodels.append(('SGD', SGDClassifier(tol=1e-3, max_iter=10000)))\nmodels.append(('BNB', BernoulliNB()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('MLP', MLPClassifier()))\nmodels.append(('LSVC', LinearSVC(max_iter=100000)))\nmodels.append(('DTC', DecisionTreeClassifier()))\nmodels.append(('ETC', ExtraTreeClassifier()))\n\nDECISION_FUNCTIONS = {\"Ridge\", \"SGD\", \"LSVC\", \"SVC\"}","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:45.771017Z","iopub.execute_input":"2022-06-13T12:37:45.771382Z","iopub.status.idle":"2022-06-13T12:37:45.789280Z","shell.execute_reply.started":"2022-06-13T12:37:45.771331Z","shell.execute_reply":"2022-06-13T12:37:45.787790Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve\n%matplotlib inline\n\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x_train1, y_train1,test_size=0.3,random_state=1)\n\nbest_model = None\nbest_model_name = \"\"\nbest_valid = 0\nfor name, model in models:\n    model.fit(x_train1, y_train1)\n    if name in DECISION_FUNCTIONS:\n        proba = model.decision_function(x_test1)\n    else:\n        proba = model.predict_proba(x_test1)[:, 1]\n    score = roc_auc_score(y_test1, proba)\n    fpr, tpr, _  = roc_curve(y_test1, proba)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', label=f\"ROC curve (auc = {score})\")\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    plt.title(f\"{name} Results\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    if score > best_valid:\n        best_valid = score\n        best_model = model\n        best_model_name = name\n\nprint(f\"Best model is {best_model_name}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:37:45.791221Z","iopub.execute_input":"2022-06-13T12:37:45.792010Z","iopub.status.idle":"2022-06-13T12:38:25.163273Z","shell.execute_reply.started":"2022-06-13T12:37:45.791961Z","shell.execute_reply":"2022-06-13T12:38:25.162603Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"colsfinal = ['x2', 'x3','x6','x7','x8','x9','x10','x11,''x12','x14','x15']\nx_train = np.array(df_train[cols])                \ny_train = np.array(df_train[\"y\"]) \nx_test = np.array(df_test[cols]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:38:25.164554Z","iopub.execute_input":"2022-06-13T12:38:25.164815Z","iopub.status.idle":"2022-06-13T12:38:25.175650Z","shell.execute_reply.started":"2022-06-13T12:38:25.164784Z","shell.execute_reply":"2022-06-13T12:38:25.174709Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# ##1st submission model Public Score = 0.97542\n# model=RandomForestClassifier(criterion='entropy',n_estimators = 900, max_depth = 9, random_state=1, oob_score=True, n_jobs=-1)\n# model.fit(x_train,y_train)\n# predictrain = model.predict(x_train)\n# prediction7 = model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:38:25.177073Z","iopub.execute_input":"2022-06-13T12:38:25.177424Z","iopub.status.idle":"2022-06-13T12:38:25.191534Z","shell.execute_reply.started":"2022-06-13T12:38:25.177384Z","shell.execute_reply":"2022-06-13T12:38:25.190308Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# ##2nd submission model Public Score = 0.99257\nclf = QuadraticDiscriminantAnalysis()\nclf.fit(x_train,y_train)\ntest_mse = clf.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:38:25.193291Z","iopub.execute_input":"2022-06-13T12:38:25.194052Z","iopub.status.idle":"2022-06-13T12:38:25.217205Z","shell.execute_reply.started":"2022-06-13T12:38:25.194004Z","shell.execute_reply":"2022-06-13T12:38:25.216103Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"t6.\"></a>\n# ‚ù§Ô∏è  6. Test and Submission","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,test_size=0.3,random_state=1)\nclf.fit(x_train, y_train)\nforest_pred = clf.predict(x_test)\nprint('\\n',confusion_matrix(y_test, forest_pred))\nprint('CLASSIFICATION REPORT\\n\\n', classification_report(y_test, forest_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:38:25.219629Z","iopub.execute_input":"2022-06-13T12:38:25.220409Z","iopub.status.idle":"2022-06-13T12:38:25.257916Z","shell.execute_reply.started":"2022-06-13T12:38:25.220355Z","shell.execute_reply":"2022-06-13T12:38:25.256925Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"submission['Expected'] =  test_mse\nsubmission.to_csv('submission.csv', index = False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T12:38:25.260183Z","iopub.execute_input":"2022-06-13T12:38:25.260978Z","iopub.status.idle":"2022-06-13T12:38:25.282800Z","shell.execute_reply.started":"2022-06-13T12:38:25.260919Z","shell.execute_reply":"2022-06-13T12:38:25.281546Z"},"trusted":true},"execution_count":35,"outputs":[]}]}